{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.time_frequency import tfr_morlet\n",
    "from alive_progress import alive_bar\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from hm_tools import *\n",
    "import scipy.io \n",
    "import h5py\n",
    "import tqdm\n",
    "import mne\n",
    "import sys\n",
    "import os\n",
    "\n",
    "mne_erp_loc = 'E:/workspace/Trust_Data_and_Results/Haoming/varibility/data/mne_erp_exp3/'\n",
    "\n",
    "result_path = 'E:/workspace/Trust_Data_and_Results/Haoming/varibility/data/mne_erp_exp3/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_band = [[8, 13]]\n",
    "frequency_band = np.array(frequency_band)\n",
    "frequency_band_name = ['alpha']\n",
    "# frequency_band = [[None, 13]]\n",
    "# frequency_band = np.array(frequency_band)\n",
    "# frequency_band_name = ['delta_theta_alpha']\n",
    "# 去除数据残缺的被试\n",
    "bad_subject = np.array([3, 5, 15]) -1\n",
    "all_subject = np.arange(0,34,1)\n",
    "\n",
    "# 删掉特定元素\n",
    "good_subject = np.setdiff1d(all_subject, bad_subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for iter_subject in good_subject:\n",
    "\n",
    "    # 生成每个被试的结果存放地址\n",
    "    subject_file = result_path + '/subject_' + str(iter_subject + 1)\n",
    "\n",
    "    # 检查地址是否存在\n",
    "    if not os.path.exists(subject_file):\n",
    "        os.makedirs(subject_file)\n",
    "\n",
    "    # read the epoch data\n",
    "    eeg_epochs = mne.read_epochs(subject_file + '/ERP_epo.fif', preload = True)\n",
    "    for iter_frequency in range(len(frequency_band_name)):\n",
    "        filtered_eeg=eeg_epochs.copy().filter(l_freq=frequency_band[iter_frequency,0], \n",
    "                                            h_freq=frequency_band[iter_frequency,1], verbose=None)\n",
    "\n",
    "        # save the mne data\n",
    "        filtered_eeg.save(subject_file +'/' + frequency_band_name[iter_frequency] + '_epo.fif', overwrite=True)\n",
    "\n",
    "    print('')\n",
    "    print(\"**************************************************************************\") \n",
    "    print(\"******************** subject number:\", iter_subject + 1, \"/\", 34,\"finished *********************\")\n",
    "    print(\"**************************************************************************\")        \n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 时间窗的长度， 单位\n",
    "time_window = 0.02  \n",
    "\n",
    "# 需要分析的数据起止时间 第一位是开始的时间，第二位是结束的时间\n",
    "involve_time = [-0.200, 1.000] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mne_erp_loc = 'E:/workspace/Trust_Data_and_Results/Haoming/varibility/data/mne_erp_exp1/'\n",
    "\n",
    "result_loc = 'E:/workspace/Trust_Data_and_Results/Haoming/varibility/result/varibilit_corr_20_exp2_new_all_step_corr/'\n",
    "\n",
    "# power band name\n",
    "file_name = ['alpha']\n",
    "\n",
    "# the number of all the subjects\n",
    "subject_num = 34\n",
    "\n",
    "# the length of time window (s)\n",
    "time_window = 0.02  \n",
    "\n",
    "# Start and end time of the data to be analyzed The first time is the start time and the second time is the end time  \n",
    "involve_time = [-0.200, 1.000] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def varibility_corr(eeg_epochs, involve_time, window_length):\n",
    "    # The time series of involve_time and epoch were compared to find out which digit started to calculate the correlation  \n",
    "    time_series = np.array(eeg_epochs.times)\n",
    "    start_time = np.where(time_series == involve_time[0])[0][0]\n",
    "    end_time = np.where(time_series == involve_time[1])[0][0]\n",
    "\n",
    "    # get eeg data from mne file\n",
    "    eeg_data = eeg_epochs.get_data()# trail * channel * time\n",
    "\n",
    "    _, not_nan_position = find_1d_array_nan(eeg_data[:,0,0])\n",
    "    eeg_data = eeg_data[not_nan_position]\n",
    "\n",
    "    # Generates an empty total result array\n",
    "    iter_subject_result = []\n",
    "\n",
    "    # Cycle each electrode once\n",
    "    for iter_elec in tqdm(range(eeg_data.shape[1])):\n",
    "\n",
    "        # Generates an empty result array for each electrode\n",
    "        iter_elec_result = []\n",
    "        # Calculate the variability once for each time window\n",
    "        for iter_window in range(end_time - start_time):# 步长为1\n",
    "            # Get the data for each time window\n",
    "            iter_data = eeg_data[:,iter_elec,(start_time + iter_window - round(window_length/2)) : (start_time + iter_window + round(window_length/2))]\n",
    "            # The correlation coefficients are calculated for the matrix \n",
    "            corr = np.corrcoef(iter_data)\n",
    "            # Get the lower triangle below the diagonal (k=-1)\n",
    "            low_triangle = np.tril(corr, k=-1)\n",
    "            # Sum of the lower trig\n",
    "            all_corr = np.sum(low_triangle)\n",
    "            # 计算下三角的数据点个数\n",
    "            corr_num = (eeg_data.shape[0]**2 - eeg_data.shape[0])/2\n",
    "            # Calculate the number of data points in the lower triangle\n",
    "            average_corr = all_corr/corr_num\n",
    "            # Calculate the variation by 1-corr\n",
    "            average_var = 1 - average_corr\n",
    "            # Store results to results for each electrode\n",
    "            iter_elec_result.append(average_var)\n",
    "        # Store results to the total result matrix\n",
    "        iter_subject_result.append(list(iter_elec_result))\n",
    "    return iter_subject_result #Output result matrix  (elec * time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter_subject in good_subject:\n",
    "\n",
    "    # Generate the eeg data storage address of each subject\n",
    "    subject_erp_loc = mne_erp_loc + '/subject_' + str(iter_subject + 1)\n",
    "\n",
    "    # Generate the storage address of the variation results of each subject\n",
    "    subject_result_loc = result_loc + '/subject_' + str(iter_subject + 1)\n",
    "    if not os.path.exists(subject_result_loc):\n",
    "        os.makedirs(subject_result_loc)\n",
    "    \n",
    "    # Alpha band only\n",
    "    for iter_file in range(len(file_name)):\n",
    "        \n",
    "        # read the epoch data\n",
    "        eeg_epochs = mne.read_epochs(subject_erp_loc +'/' + file_name[iter_file] + '_epo.fif', preload = True)\n",
    "\n",
    "        # Check the number of event\n",
    "        event_name = eeg_epochs.event_id.keys()\n",
    "        event_name = list(event_name)\n",
    "\n",
    "        # create the dict for save the result\n",
    "        var_result = {}\n",
    "\n",
    "        # input some basic information in the dict result\n",
    "        var_result['ch_names'] = eeg_epochs.ch_names\n",
    "        var_result['fs'] = eeg_epochs.info['sfreq']\n",
    "        \n",
    "        window_length = eeg_epochs.info['sfreq'] * time_window \n",
    "        print('point of the window is:', window_length)\n",
    "\n",
    "        var_erp_all = varibility_corr(eeg_epochs, involve_time, window_length)\n",
    "\n",
    "        # save the all_event result to the dict\n",
    "        var_result['all_event'] = var_erp_all    \n",
    "\n",
    "        # Calculate variation for each event\n",
    "        for iter_event in range(len(event_name)):\n",
    "\n",
    "            var_erp_iter_event = varibility_corr(eeg_epochs[event_name[iter_event]], involve_time, window_length)\n",
    "\n",
    "            # save the result per event\n",
    "            var_result[event_name[iter_event]] = var_erp_iter_event\n",
    "\n",
    "\n",
    "        # save the mne data\n",
    "        np.save(subject_result_loc +'/' + file_name[iter_file] + '_corr.npy', var_result)\n",
    "\n",
    "    print('')\n",
    "    print(\"**************************************************************************\") \n",
    "    print(\"******************** subject number:\", iter_subject + 1, \"/\", subject_num,\"finished *********************\")\n",
    "    print(\"**************************************************************************\")        \n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_loc = 'E:/workspace/Trust_Data_and_Results/Haoming/varibility/result/varibilit_corr_20_exp2_new_all_step_corr/'\n",
    "\n",
    "# Extract the data from each participant’s folder and integrate them into a large matrix\n",
    "all_data = []\n",
    "for iter_subject in good_subject:\n",
    "\n",
    "    # Generate the erp storage address of each subject\n",
    "    subject_erp_loc = var_loc + '/subject_' + str(iter_subject + 1)\n",
    "\n",
    "    iter_subject_data = []\n",
    "    for iter_file in range(len(file_name)):\n",
    "\n",
    "        file_loc = subject_erp_loc + '/' + file_name[iter_file] + '_corr.npy'\n",
    "\n",
    "        var_result = np.load(file_loc, allow_pickle=True).item()\n",
    "\n",
    "        iter_file_data = []\n",
    "        for inter_event in range(len(event_name)):\n",
    "\n",
    "            iter_data = var_result[event_name[inter_event]]\n",
    "            iter_file_data.append(list(iter_data))\n",
    "        \n",
    "        iter_subject_data.append(iter_file_data)\n",
    "    #iter_subject_data.shape\n",
    "    all_data.append(iter_subject_data)\n",
    "\n",
    "all_data = np.array(all_data)\n",
    "all_data.shape\n",
    "            \n",
    "\n",
    "all_var_result = {}\n",
    "all_var_result['shape'] = ['subjects', 'power_band', 'event_name', 'ch_names', 'time_point']\n",
    "all_var_result['power_band'] = file_name\n",
    "all_var_result['event_name'] = event_name\n",
    "all_var_result['ch_names'] = var_result['ch_names']\n",
    "all_var_result['fs'] = var_result['fs']\n",
    "all_var_result['data'] = all_data\n",
    "np.save(result_loc + '/all_var_data.npy', all_var_result)\n",
    "\n",
    "# normalize all the data by divide the by -200 ~ 0s\n",
    "all_data_nor = list([])\n",
    "for iter_subject in range(31):\n",
    "\n",
    "    # 生成每个被试的erp存放地址\n",
    "    subject_erp_loc = var_loc + '/subject_' + str(iter_subject + 1)\n",
    "\n",
    "    iter_subject_data_nor = []\n",
    "    for iter_file in range(len(file_name)):\n",
    "\n",
    "        iter_file_data_nor = []\n",
    "        for iter_event in range(len(event_name)):\n",
    "            \n",
    "            iter_elec_data_nor=[]\n",
    "            for iter_elec in range(63):\n",
    "\n",
    "                var_data = np.squeeze(all_data[iter_subject, iter_file, iter_event, iter_elec])\n",
    "                var_data_nor = var_data / np.mean(var_data[0:200])\n",
    "                iter_elec_data_nor.append(list(var_data_nor))\n",
    "\n",
    "            iter_file_data_nor.append(list(iter_elec_data_nor))\n",
    "        \n",
    "        iter_subject_data_nor.append(iter_file_data_nor)\n",
    "    #iter_subject_data.shape\n",
    "    all_data_nor.append(iter_subject_data_nor)\n",
    "\n",
    "all_data_nor = np.array(all_data_nor)\n",
    "all_data_nor.shape\n",
    "\n",
    "\n",
    "all_var_result_nor = {}\n",
    "all_var_result_nor['shape'] = ['subjects', 'power_band', 'event_name', 'ch_names', 'time_point']\n",
    "all_var_result_nor['power_band'] = file_name\n",
    "all_var_result_nor['event_name'] = event_name\n",
    "all_var_result_nor['ch_names'] = var_result['ch_names']\n",
    "all_var_result_nor['fs'] = var_result['fs']\n",
    "all_var_result_nor['data'] = all_data_nor\n",
    "all_var_result_nor['tmin'] = -0.2\n",
    "all_data_nor = all_var_result_nor\n",
    "np.save(result_loc + '/all_var_data_nor.npy', all_var_result_nor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个只计算三种情况相对于0的test\n",
    "permu_result_loc = result_loc + '/permutation_result/'\n",
    "n_permutations=10000\n",
    "permutation_cluster_result = {}\n",
    "# calculate std\n",
    "\n",
    "tfr_ROI_epoch_data_std={}\n",
    "tfr_ROI_epoch_data_std['all_event_std'] = np.std(np.squeeze(all_data_nor['data'][:, :, 0, :, :]), axis=0)\n",
    "tfr_ROI_epoch_data_std['event1_std'] = np.std(np.squeeze(all_data_nor['data'][:, :, 1, :, :]), axis=0)\n",
    "tfr_ROI_epoch_data_std['event2_std'] = np.std(np.squeeze(all_data_nor['data'][:, :, 2, :, :]), axis=0)\n",
    "\n",
    "permutation_cluster_result['std_error'] = tfr_ROI_epoch_data_std\n",
    "\n",
    "print('')\n",
    "print(\"**************************************************************************\") \n",
    "#print(\"*\"*10)\n",
    "print(\"************************ std calculation finished ************************\")\n",
    "print(\"**************************************************************************\")        \n",
    "print('')\n",
    "\n",
    "      \n",
    "for ROI_num in range(63):\n",
    "    #ROI_num = 10\n",
    "    # compute the cluster test for event 1\n",
    "    T_obs, clusters, cluster_p_values, H0  = mne.stats.permutation_cluster_1samp_test(all_data_nor['data'][:, 3, 1, ROI_num, :]-1, out_type='mask',n_permutations=n_permutations, tail=0, verbose=None)\n",
    "    event1_result = {'T_obs':T_obs, 'clusters':clusters, 'cluster_p_values':cluster_p_values, 'H0':H0}\n",
    "\n",
    "    # compute the cluster test for event 2\n",
    "    T_obs, clusters, cluster_p_values, H0  = mne.stats.permutation_cluster_1samp_test(all_data_nor['data'][:, 3, 2, ROI_num, :]-1, out_type='mask',n_permutations=n_permutations, tail=0, verbose=None)\n",
    "    event2_result = {'T_obs':T_obs, 'clusters':clusters, 'cluster_p_values':cluster_p_values, 'H0':H0}\n",
    "\n",
    "    # compute the cluster test for  all event \n",
    "    T_obs, clusters, cluster_p_values, H0  = mne.stats.permutation_cluster_1samp_test(all_data_nor['data'][:, 3, 0, ROI_num, :]-1, out_type='mask',n_permutations=n_permutations, tail=0, verbose=None)\n",
    "    all_event_result = {'T_obs':T_obs, 'clusters':clusters, 'cluster_p_values':cluster_p_values, 'H0':H0}\n",
    "\n",
    "    # put them in a dict\n",
    "    permutation_cluster_result_per_roi = {'event1_result':event1_result, 'event2_result':event2_result, 'all_event_result':all_event_result}\n",
    "    permutation_cluster_result[ROI_num] = permutation_cluster_result_per_roi\n",
    "\n",
    "\n",
    "    print('')\n",
    "    print(\"**************************************************************************\")\n",
    "    print(\"********************* total number:\", ROI_num + 1, \"/\", 63,\"finished ***********************\")\n",
    "    print(\"**************************************************************************\")\n",
    "    print('')\n",
    "\n",
    "if not os.path.exists(permu_result_loc):\n",
    "    os.makedirs(permu_result_loc)\n",
    "np.save(permu_result_loc + 'all_event_0_1_alpha' + '.npy', permutation_cluster_result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "def plot_erp(permutation_cluster_result, epoch_mean, epoch_data_std, times, event_name, line_color=['orangered','limegreen'], \n",
    "            figsize=(14,6), title_size=20, legend_size=15, labelsize=15, ticksize=20, subplots_adjust=[0.15, 0.15, 0.85, 0.85]):\n",
    "    # keys of the dict permutation_cluster_result\n",
    "    #'event1_result':event1_result, 'event2_result':event2_result, 'compare_result':compare_result}\n",
    "    #'T_obs':T_obs, 'clusters':clusters, 'cluster_p_values':cluster_p_values, 'H0':H0\n",
    "\n",
    "    # keys of the dict epoch_data_mean\n",
    "    # 'event_0' (34, 1300)\n",
    "    # plot for each ROI\n",
    "\n",
    "    event_0_line_color = line_color[0]\n",
    "    event_1_line_color = line_color[1]\n",
    "    event_2_line_color = line_color[2]\n",
    "\n",
    "\n",
    "    plt.close('all')\n",
    "    plt.rcParams['figure.figsize'] = figsize # 设置figure_size尺寸\n",
    "\n",
    "    plt.plot(times, epoch_mean['all_event'], color=event_0_line_color, linestyle='--', alpha=0.4)\n",
    "    plt.plot(times, epoch_mean['event_0'], color=event_1_line_color, linestyle='--', alpha=0.4)\n",
    "    plt.plot(times, epoch_mean['event_1'], color=event_2_line_color, linestyle='--', alpha=0.4)\n",
    "\n",
    "\n",
    "    # Plot std\n",
    "    std_event0 = epoch_data_std['all_event']\n",
    "    std_event1 = epoch_data_std['event_0']\n",
    "    std_all_event = epoch_data_std['event_1']\n",
    "\n",
    "    plt.fill_between(times, epoch_mean['all_event'] - std_all_event, epoch_mean['all_event'] + std_all_event, color=event_0_line_color, alpha=0.1)\n",
    "    plt.fill_between(times, epoch_mean['event_0'] - std_event0, epoch_mean['event_0'] + std_event0, color=event_1_line_color, alpha=0.1)\n",
    "    plt.fill_between(times, epoch_mean['event_1'] - std_event1, epoch_mean['event_1'] + std_event1, color=event_2_line_color, alpha=0.1)\n",
    "\n",
    "    # Plot the significant of event 0 \n",
    "    for i_c, c in enumerate(permutation_cluster_result['all_event_result']['clusters']):\n",
    "        c = c[0]\n",
    "        if permutation_cluster_result['all_event_result']['cluster_p_values'][i_c] <= 0.05:\n",
    "            plt.plot(times[c.start : c.stop - 1], epoch_mean['all_event'][c.start : c.stop-1], color=event_0_line_color, alpha=0.9)\n",
    "\n",
    "    # Plot the significant of event 1 \n",
    "    for i_c, c in enumerate(permutation_cluster_result['event1_result']['clusters']):\n",
    "        c = c[0]\n",
    "        if permutation_cluster_result['event1_result']['cluster_p_values'][i_c] <= 0.05:\n",
    "            plt.plot(times[c.start : c.stop - 1], epoch_mean['event_0'][c.start : c.stop-1], color=event_1_line_color, alpha=0.9)\n",
    "\n",
    "    # Plot the significant between event 1 and 2   \n",
    "    for i_c, c in enumerate(permutation_cluster_result['event2_result']['clusters']):\n",
    "        c = c[0]\n",
    "        if permutation_cluster_result['event2_result']['cluster_p_values'][i_c] <= 0.05:\n",
    "            plt.plot(times[c.start : c.stop - 1], epoch_mean['event_1'][c.start : c.stop-1], color=event_2_line_color, alpha=0.9)\n",
    "\n",
    "    plt.subplots_adjust(left=subplots_adjust[0], bottom=subplots_adjust[1], right=subplots_adjust[2], top=subplots_adjust[3],                         hspace=0.1,wspace=0.1)\n",
    "\n",
    "    plt.xlim([times[0]-0.02, times[-1]+0.02])\n",
    "\n",
    "    plt.yticks(size=ticksize, family='Arial')\n",
    "    plt.xticks(size=ticksize, family='Arial')\n",
    "\n",
    "\n",
    "    return plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot varibility erps\n",
    "plot_result_loc = result_loc + '/alpha_plot_result/'\n",
    "permutation_cluster_result = np.load(permu_result_loc + 'all_event_0_1_alpha' + '.npy', allow_pickle=True).item()\n",
    "\n",
    "\n",
    "figsize=(10,6)\n",
    "title_size = 20\n",
    "labelsize = 15\n",
    "ticksize=25\n",
    "# all_data_nor = np.load(var_loc + '/all_var_data_nor_3band.npy', allow_pickle=True).item()\n",
    "\n",
    "# for iter_file in range(len(file_name)):\n",
    "\n",
    "\n",
    "if not os.path.exists(plot_result_loc):\n",
    "    os.makedirs(plot_result_loc)\n",
    "\n",
    "# iter_permutation_cluster_result = np.load(permu_result_loc + file_name[iter_file] + '.npy', allow_pickle=True).item()\n",
    "for iter_elec in range(63):\n",
    "\n",
    "    epoch_data_std = {}\n",
    "    epoch_data_std['all_event'] = np.std(np.squeeze(all_data_nor['data'][:, 3, 0, iter_elec, :]), axis=0)\n",
    "    epoch_data_std['event_0'] = np.std(np.squeeze(all_data_nor['data'][:, 3, 1, iter_elec, :]), axis=0)\n",
    "    epoch_data_std['event_1'] = np.std(np.squeeze(all_data_nor['data'][:, 3, 2, iter_elec, :]), axis=0)\n",
    "\n",
    "    epoch_data_mean = {}\n",
    "    epoch_data_mean['all_event'] = np.average(np.squeeze(all_data_nor['data'][:, 3, 0, iter_elec, :]), axis=0)\n",
    "    epoch_data_mean['event_0'] = np.average(np.squeeze(all_data_nor['data'][:, 3, 1, iter_elec, :]), axis=0)\n",
    "    epoch_data_mean['event_1'] = np.average(np.squeeze(all_data_nor['data'][:, 3, 2, iter_elec, :]), axis=0)\n",
    "\n",
    "    times = np.arange(-0.2, 1, 0.001)\n",
    "    event_name = ['Trust & Untrust', 'Trust', 'Untrust']\n",
    "\n",
    "\n",
    "    plt = plot_erp(permutation_cluster_result[iter_elec], epoch_data_mean, epoch_data_std, \n",
    "    times, event_name, line_color=['#DF4058','#70AD47','#0070C0'],\n",
    "    ticksize = ticksize, figsize= figsize)\n",
    "\n",
    "    plt.title(all_data_nor['ch_names'][iter_elec], fontdict= {'fontsize':title_size})\n",
    "\n",
    "    plt.xlabel(\"time (s)\", fontsize=labelsize)\n",
    "    plt.ylabel(\"varibility change\", fontsize=labelsize)\n",
    "\n",
    "    # plot the cross line\n",
    "    plt.axvline(times[201], c=\"gray\", ls = \"dashed\")\n",
    "    plt.plot(times, np.ones(len(times)), color=\"gray\", linestyle=\"dashed\")\n",
    "\n",
    "    plt.savefig(plot_result_loc + '/' + all_data_nor['ch_names'][iter_elec] + \".png\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
